{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b5d73-cca6-4974-8977-9de37814cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1):-\n",
    "Grid search CV (Cross-Validation) is a technique used in machine learning to tune hyperparameters for a given model.\n",
    "The purpose of grid search CV is to exhaustively search for the best combination of hyperparameters from a predefined grid of parameter values.\n",
    "\n",
    "Hyperparameters are settings that are not learned from the data but are set prior to the learning process. \n",
    "They control the behavior of the learning algorithm and can significantly impact the performance of the model.\n",
    "Examples of hyperparameters include the learning rate, regularization parameter, maximum tree depth, and so on.\n",
    "\n",
    "Grid search CV works by defining a grid of hyperparameter values to explore. For each combination of hyperparameters,\n",
    "the model is trained and evaluated using a specified evaluation metric, such as accuracy, precision, or mean squared error. \n",
    "The evaluation is typically done using cross-validation, where the training data is split into multiple folds, and the model is trained and \n",
    "tested on different combinations of these folds. This helps to obtain a more robust estimate of the model's performance.\n",
    "\n",
    "The process can be summarized in the following steps:\n",
    "\n",
    "Define the model: Select the machine learning algorithm you want to use and define its structure.\n",
    "\n",
    "Define the hyperparameter grid: Create a dictionary or a list of hyperparameters and their corresponding values that you want to explore.\n",
    "For example, if you are using a decision tree, you might specify different values for the maximum depth, minimum samples split, and so on.\n",
    "\n",
    "Perform grid search CV: Iterate over all possible combinations of hyperparameters and train/evaluate the model for each combination. \n",
    "This typically involves fitting the model on the training data, evaluating it on the validation data, and recording the performance metric.\n",
    "\n",
    "Select the best combination: After evaluating all combinations, choose the combination of hyperparameters that yielded the best performance on\n",
    "the validation data. This could be the combination with the highest accuracy or the lowest error, depending on the problem.\n",
    "\n",
    "Evaluate on unseen data: Once you have selected the best combination of hyperparameters, you can evaluate the model's performance on a separate set \n",
    "of unseen test data to get an unbiased estimate of how well it generalizes to new data.\n",
    "\n",
    "Grid search CV helps to automate the process of hyperparameter tuning by systematically searching through the predefined grid. It saves time and\n",
    "effort compared to manually testing different combinations. However, it can be computationally expensive if the grid is large or the dataset is large, \n",
    "as it requires training and evaluating multiple models. Alternative methods, such as random search or Bayesian optimization, can be used to address \n",
    "these challenges and improve the efficiency of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e3a1c-d342-4f03-ad2f-893cbdf3395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2):-\n",
    "Grid search CV and random search CV are both techniques used for hyperparameter tuning in machine learning, but they differ in the way they explore\n",
    "the hyperparameter space. Here are the key differences between the two:\n",
    "\n",
    "Search Strategy:\n",
    "\n",
    "Grid Search CV: Grid search exhaustively searches through all possible combinations of hyperparameters within a predefined grid. It systematically \n",
    "explores each point in the grid.\n",
    "Random Search CV: Random search randomly selects hyperparameter values from a predefined distribution. It samples the hyperparameter space randomly.\n",
    "Exploration of Hyperparameter Space:\n",
    "\n",
    "Grid Search CV: Grid search covers the entire grid, meaning it tests every combination of hyperparameters. It is a brute-force approach that \n",
    "guarantees to find the optimal combination within the specified grid, assuming the grid is fine-grained enough.\n",
    "Random Search CV: Random search explores the hyperparameter space by randomly sampling hyperparameters. It focuses on a subset of the hyperparameter \n",
    "space but provides the flexibility to explore a wider range of values.\n",
    "\n",
    "Efficiency:\n",
    "Grid Search CV: Grid search can be computationally expensive, especially when dealing with a large number of hyperparameters or a large dataset.\n",
    "As it evaluates every combination, the search space grows exponentially, making it more time-consuming.\n",
    "Random Search CV: Random search is more efficient than grid search because it randomly samples hyperparameters, allowing for a more diverse \n",
    "exploration of the hyperparameter space. It can potentially find good hyperparameter combinations with fewer evaluations.\n",
    "When to choose one over the other:\n",
    "\n",
    "Grid Search CV: Grid search is useful when you have a good understanding of the hyperparameters and their possible values. \n",
    "It is suitable when the hyperparameter space is relatively small and the computational resources are sufficient to explore all combinations.\n",
    "Grid search is also preferable when there is prior knowledge or evidence suggesting specific hyperparameter values to be tested.\n",
    "\n",
    "Random Search CV: Random search is beneficial when the hyperparameter space is large or not well understood. It provides a more efficient way \n",
    "to explore a wide range of hyperparameters. Random search is often a good choice when computational resources are limited or when you have a hunch\n",
    "that good hyperparameter combinations are scattered across the space.\n",
    "\n",
    "In general, random search CV is more commonly used due to its flexibility and efficiency in finding good hyperparameter combinations.\n",
    "However, there might be cases where grid search CV is preferred, such as when the hyperparameter space is small and known to be critical, \n",
    "or when you want to ensure a systematic exploration of the entire grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae30d54-eabd-4f62-af7d-0e4f50ebc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3):-\n",
    "Data leakage refers to the situation in machine learning where information from the test set or future data inadvertently leaks into the training \n",
    "process. It occurs when features, information, or knowledge that would not be available at the time of prediction are included during model training\n",
    "or evaluation. Data leakage can lead to overly optimistic performance metrics and unreliable models that fail to generalize well to new, unseen data.\n",
    "\n",
    "Data leakage is a problem in machine learning because it undermines the integrity and validity of model evaluation and can lead to misleading results.\n",
    "It can make a model appear more accurate than it actually is, as it may exploit information that will not be available at the time of making \n",
    "predictions. When deployed in real-world scenarios, such models are likely to perform poorly and make incorrect predictions.\n",
    "\n",
    "Here's an example to illustrate data leakage:\n",
    "\n",
    "Let's say you are building a credit card fraud detection model. You have a dataset that includes various features such as transaction amount,\n",
    "location, time, and a target variable indicating whether the transaction is fraudulent or not. You decide to include a feature called \"is_fraudulent\" \n",
    "hat is derived from the target variable. It indicates whether a transaction in the dataset is fraudulent or not based on the target variable.\n",
    "\n",
    "During model training, if you include the \"is_fraudulent\" feature, you are effectively providing the model with direct information about the target\n",
    "variable that it should predict. As a result, the model will easily learn to associate this feature with the target variable and achieve high accuracy\n",
    "during training.\n",
    "\n",
    "However, this feature introduces data leakage. In real-world scenarios, at the time of prediction, you will not have access to the target variable or\n",
    "any information about whether a transaction is fraudulent or not. Therefore, including this feature in the training process creates a situation\n",
    "where the model is learning from future data or information it would not have at the time of making predictions.\n",
    "The model's performance will be artificially inflated during training, and it will likely fail to generalize well when deployed.\n",
    "\n",
    "To avoid data leakage, it is crucial to ensure that the training data only includes features that would be available at the time of \n",
    "making predictions. Features derived from the target variable or any other future information should be excluded from the \n",
    "training process to obtain reliable and unbiased models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318de44-de7d-4509-ab99-eef086f73ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4):-\n",
    "To prevent data leakage when building a machine learning model, you can follow several best practices:\n",
    "\n",
    "Split the data properly: Ensure that you split your data into separate sets for training, validation, and testing. The training set is used for model\n",
    "training, the validation set is used for hyperparameter tuning and model evaluation, and the testing set is used for final model evaluation.\n",
    "Data leakage can occur if information from the validation or testing set leaks into the training set.\n",
    "\n",
    "Feature selection and engineering: Perform feature selection and engineering based only on the information available at the time of prediction.\n",
    "Avoid using features that incorporate future or target-related information that would not be accessible during deployment. It's important to create \n",
    "features that reflect real-world scenarios and avoid including features that provide direct access to the target variable or future information.\n",
    "\n",
    "Use appropriate cross-validation techniques: If you use cross-validation for model evaluation, ensure that the validation data in each fold does not \n",
    "leak information from the test set or future data. Stratified K-fold cross-validation and Time series cross-validation are commonly used techniques\n",
    "that help prevent leakage by preserving the temporal or distributional characteristics of the data.\n",
    "\n",
    "Be cautious with data preprocessing steps: Preprocessing steps such as scaling, normalization, or imputation should be performed separately on the \n",
    "training and testing/validation sets. Calculating statistics (e.g., mean, standard deviation) should be based only on the training data and then \n",
    "applied consistently to the testing/validation data.\n",
    "\n",
    "Be aware of the temporal nature of the data: In time-series or sequential data, ensure that the training data comes before the validation and testing \n",
    "data. This is crucial to maintain the temporal order and avoid any leakage from future data into the training set.\n",
    "\n",
    "Understand the data collection process: Gain a thorough understanding of how the data was collected and processed. Be mindful of any potential sources\n",
    "of leakage, such as data collection biases or accidental inclusion of future information.\n",
    "\n",
    "Regularly evaluate model performance: Monitor your model's performance on unseen data or in a real-world setting to ensure that it generalizes well\n",
    "and does not suffer from data leakage. Continuously validate the model's accuracy, precision, or other performance metrics to identify any signs of\n",
    "leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1b74a-b8eb-4c4e-8e21-9c7068d53505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5:-\n",
    "A confusion matrix is a table that summarizes the performance of a classification model on a set of test data. It provides a detailed breakdown of\n",
    "the model's predictions compared to the true labels of the data. The confusion matrix is particularly useful in evaluating the performance of binary \n",
    "or multi-class classification models.\n",
    "\n",
    "A confusion matrix has four key components:\n",
    "\n",
    "True Positives (TP): It represents the number of instances correctly predicted as positive (or the target class) by the model.\n",
    "\n",
    "True Negatives (TN): It represents the number of instances correctly predicted as negative (or the non-target class) by the model.\n",
    "\n",
    "False Positives (FP): It represents the number of instances incorrectly predicted as positive by the model when they are actually negative. \n",
    "These are also known as Type I errors or false alarms.\n",
    "\n",
    "False Negatives (FN): It represents the number of instances incorrectly predicted as negative by the model when they are actually positive.\n",
    "These are also known as Type II errors or misses.\n",
    "\n",
    "With these components, the confusion matrix typically looks like this for binary classification:\n",
    "\n",
    "mathematica\n",
    "Copy code\n",
    "               Predicted Negative    Predicted Positive\n",
    "Actual Negative       TN                      FP\n",
    "Actual Positive       FN                      TP\n",
    "\n",
    "The confusion matrix provides several important metrics for evaluating the performance of a classification model:\n",
    "\n",
    "Accuracy: It measures the overall correctness of the model's predictions and is calculated as (TP + TN) / (TP + TN + FP + FN). \n",
    "It represents the proportion of correctly classified instances.\n",
    "\n",
    "Precision: It measures the model's ability to correctly identify positive instances among the predicted positives and is calculated as \n",
    "TP / (TP + FP). It focuses on the quality of the positive predictions.\n",
    "\n",
    "Recall (also known as sensitivity or true positive rate): It measures the model's ability to correctly identify positive instances among\n",
    "the actual positives and is calculated as TP / (TP + FN). It focuses on the coverage of the positive instances.\n",
    "\n",
    "Specificity: It measures the model's ability to correctly identify negative instances among the actual negatives and is calculated as \n",
    "TN / (TN + FP). It is the complement of the false positive rate.\n",
    "\n",
    "F1 score: It is the harmonic mean of precision and recall and provides a single metric that balances both measures.\n",
    "It is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "The confusion matrix and these metrics collectively provide insights into the performance of the classification model, highlighting its accuracy,\n",
    "ability to correctly identify positive or negative instances, and potential trade-offs between precision and recall.\n",
    "They help assess the model's strengths and weaknesses, aiding in model selection and decision-making in various applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070151d-7430-4892-9b14-9fa49dd7dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6):-\n",
    "Precision and recall are two performance metrics used to evaluate the effectiveness of a classification model, particularly in situations where the\n",
    "class distribution is imbalanced. Both metrics are derived from the confusion matrix and provide insights into different aspects of the model's \n",
    "performance.\n",
    "\n",
    "Precision: Precision measures the model's ability to correctly identify positive instances among the instances predicted as positive.\n",
    "It focuses on the quality of the positive predictions made by the model. Precision is calculated as TP / (TP + FP), where TP is the number of true \n",
    "positives and FP is the number of false positives.\n",
    "\n",
    "\n",
    "A high precision score indicates that the model has a low rate of false positives, meaning it rarely misclassifies negative instances as positive. \n",
    "It implies that when the model predicts an instance as positive, it is likely to be correct. Precision is particularly useful in situations where the\n",
    "cost of false positives is high, such as medical diagnoses or fraud detection, where incorrectly labeling negative instances as positive can have \n",
    "severe consequences.\n",
    "\n",
    "Recall: Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify positive instances among all the\n",
    "actual positive instances. It focuses on the coverage of the positive instances by the model. Recall is calculated as TP / (TP + FN), where TP is the \n",
    "number of true positives and FN is the number of false negatives.\n",
    "\n",
    "A high recall score indicates that the model can effectively identify most of the positive instances in the dataset.\n",
    "It means the model has a low rate of false negatives, suggesting that it rarely misses positive instances. Recall is important in situations\n",
    "where the cost of false negatives is high, such as disease diagnosis, where failing to detect a positive instance can have severe consequences.\n",
    "\n",
    "To summarize:\n",
    "Precision focuses on the quality of positive predictions, specifically the ability to minimize false positives.\n",
    "Recall focuses on the coverage of positive instances, specifically the ability to minimize false negatives.\n",
    "Depending on the specific problem and its associated costs and consequences, the importance of precision and recall may vary. \n",
    "In some cases, high precision may be more important, while in others, high recall may be the priority. It is crucial to consider the specific \n",
    "requirements and trade-offs of the application domain when interpreting and optimizing these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa68bf2f-387a-4bc4-878f-c381da376894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7):-\n",
    "To interpret a confusion matrix and determine which types of errors your model is making, you can analyze the values in the matrix and observe the \n",
    "patterns within each cell. Here's how you can interpret the confusion matrix to gain insights into the model's errors:\n",
    "\n",
    "True Positives (TP): TP represents the number of instances that are correctly predicted as positive by the model. These are the instances where \n",
    "the model predicted the positive class, and they are indeed positive. TP indicates the model's correct predictions of the positive class.\n",
    "\n",
    "True Negatives (TN): TN represents the number of instances that are correctly predicted as negative by the model. These are the instances where \n",
    "the model predicted the negative class, and they are indeed negative. TN indicates the model's correct predictions of the negative class.\n",
    "\n",
    "False Positives (FP): FP represents the number of instances that are incorrectly predicted as positive by the model. These are the instances where \n",
    "the model predicted the positive class, but they are actually negative. FP indicates the instances where the model made a false positive error,\n",
    "also known as Type I errors or false alarms.\n",
    "\n",
    "False Negatives (FN): FN represents the number of instances that are incorrectly predicted as negative by the model. These are the instances\n",
    "where the model predicted the negative class, but they are actually positive. FN indicates the instances where the model made a false negative error,\n",
    "also known as Type II errors or misses.\n",
    "\n",
    "By analyzing the values in the confusion matrix, you can gain insights into the types of errors the model is making:\n",
    "\n",
    "High FP (False Positives): If you observe a relatively high number of false positives, it means the model is incorrectly predicting negative \n",
    "instances as positive. This can suggest that the model has a tendency to generate false alarms or classify instances as positive when they should be\n",
    "negative.\n",
    "\n",
    "High FN (False Negatives): If you observe a relatively high number of false negatives, it means the model is incorrectly predicting positive\n",
    "instances as negative. This indicates that the model is missing positive instances or failing to detect them.\n",
    "\n",
    "Balanced FP and FN: If the number of false positives and false negatives is both relatively low, it suggests that the model is performing well \n",
    "in terms of both positive and negative predictions.\n",
    "\n",
    "Interpreting the confusion matrix in this way helps identify the specific types of errors the model is making. These insights can guide further\n",
    "analysis, fine-tuning of the model, or considering the associated costs and consequences of different error types in the specific domain or problem \n",
    "context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46c49e-71c1-44bb-b7f8-b47952708765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8):-\n",
    "Several common metrics can be derived from a confusion matrix to evaluate the performance of a classification model. Here are some key metrics and \n",
    "their calculation methods:\n",
    "\n",
    "Accuracy: Accuracy measures the overall correctness of the model's predictions and is calculated as:\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision: Precision measures the model's ability to correctly identify positive instances among the instances predicted as positive.\n",
    "It is calculated as:\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the model's ability to correctly identify positive instances among all the actual\n",
    "positive instances. It is calculated as:\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "Specificity (True Negative Rate): Specificity measures the model's ability to correctly identify negative instances among all the actual negative \n",
    "instances. It is calculated as:\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both measures. It is calculated as:\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "False Positive Rate (Fallout): The false positive rate measures the proportion of negative instances that are incorrectly classified as positive. \n",
    "It is calculated as:\n",
    "False Positive Rate = FP / (FP + TN)\n",
    "\n",
    "False Negative Rate: The false negative rate measures the proportion of positive instances that are incorrectly classified as negative. \n",
    "It is calculated as:\n",
    "False Negative Rate = FN / (FN + TP)\n",
    "\n",
    "True Negative Rate (Specificity): The true negative rate is synonymous with specificity and measures the proportion of negative instances that are\n",
    "correctly classified as negative. It is calculated as:\n",
    "True Negative Rate = TN / (TN + FP)\n",
    "\n",
    "These metrics provide different perspectives on the performance of a classification model. Accuracy provides an overall assessment of correctness,\n",
    "while precision and recall focus on positive predictions. Specificity and false positive rate provide insights into negative predictions. \n",
    "The F1 score balances precision and recall, considering both types of errors.\n",
    "\n",
    "When interpreting these metrics, it is important to consider the specific problem context, class imbalances, and the associated costs and\n",
    "consequences of different types of errors. The choice of which metrics to prioritize depends on the specific requirements and trade-offs of the \n",
    "application domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d99d93-b0ae-4f16-93a1-86d4b92a87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9):-\n",
    "The accuracy of a model represents the overall correctness of its predictions and is calculated as the ratio of correct predictions to the total\n",
    "number of predictions. It is an important performance metric, but it does not provide a complete picture of the model's performance, especially in\n",
    "scenarios with imbalanced class distributions or when the cost of different types of errors varies.\n",
    "\n",
    "The relationship between the accuracy of a model and the values in its confusion matrix can be understood by examining how the accuracy is influenced \n",
    "by true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "Accuracy is calculated as:\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "The accuracy metric takes into account both correct predictions (TP and TN) and incorrect predictions (FP and FN) made by the model.\n",
    "However, it treats all types of errors equally.\n",
    "\n",
    "In a balanced dataset where the class distribution is roughly equal, accuracy is a meaningful metric. If the number of TP and TN is high relative \n",
    "to the number of FP and FN, the accuracy score will be high, indicating that the model performs well in both positive and negative predictions.\n",
    "\n",
    "However, in cases where the class distribution is imbalanced or the cost of different types of errors differs significantly, accuracy can be \n",
    "misleading. It may provide a falsely optimistic evaluation of the model's performance. For example, if the dataset contains a majority of negative \n",
    "instances and the model tends to predict the negative class for most instances, the accuracy can be high even if the model fails to detect positive\n",
    "instances effectively (high FN).\n",
    "\n",
    "\n",
    "The values in the confusion matrix, on the other hand, provide a more detailed breakdown of the model's predictions and highlight specific types of\n",
    "errors made by the model. By analyzing the confusion matrix, you can gain insights into false positives, false negatives, true positives, and true\n",
    "negatives, which help assess the model's performance more comprehensively.\n",
    "\n",
    "In summary, while accuracy is a commonly used metric to evaluate a model's performance, it should be interpreted with caution, especially \n",
    "when dealing with imbalanced datasets or when different types of errors have varying costs or consequences. Examining the values in the confusion \n",
    "matrix provides a more granular understanding of the model's predictions and the types of errors it makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c73ca-2aac-40ac-9515-fb962250a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10):-\n",
    "A confusion matrix can be a valuable tool for identifying potential biases or limitations in your machine learning model. By analyzing the values in\n",
    "the confusion matrix, you can gain insights into specific patterns of errors, which may indicate biases or limitations. Here's how you can use a\n",
    "confusion matrix for this purpose:\n",
    "\n",
    "Class Imbalance: Check if there is a significant disparity in the number of instances between the classes. If one class dominates the dataset,\n",
    "the model may exhibit a bias towards that class, leading to imbalanced predictions.\n",
    "\n",
    "\n",
    "False Positives and False Negatives: Examine the number of false positives (FP) and false negatives (FN) in the confusion matrix. \n",
    "If there is a notable imbalance between these error types, it suggests that the model may have a bias towards either making false positive errors or \n",
    "false negative errors. This can indicate a specific limitation or bias in the model's decision-making process.\n",
    "\n",
    "Error Disparity: Compare the performance metrics (such as precision and recall) for different classes in the confusion matrix. If there are\n",
    "significant disparities in these metrics across classes, it could indicate biases or limitations in the model's ability to generalize well to\n",
    "certain classes. It is essential to examine if the model is consistently underperforming on specific classes or exhibiting significantly different \n",
    "error rates.\n",
    "\n",
    "Confusion Between Similar Classes: If you have multiple similar classes in your classification problem, examine if the model is frequently confusing\n",
    "these classes. For example, if the model consistently misclassifies instances from one class as another, it could indicate a limitation in capturing\n",
    "the distinguishing characteristics between those classes.\n",
    "\n",
    "Analysis Across Subgroups: If your data includes subgroups or different demographic categories, you can compute separate confusion matrices for each \n",
    "subgroup. This analysis can help identify if the model's performance varies across different groups, which may suggest the presence of bias or \n",
    "limitations in the model's generalization across subgroups.\n",
    "\n",
    "By leveraging the insights from the confusion matrix, you can identify potential biases or limitations in your machine learning model. \n",
    "These findings can guide further investigation, data collection improvements, feature engineering, or algorithmic adjustments to address the \n",
    "identified issues and improve the model's performance across different classes or subgroups."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
